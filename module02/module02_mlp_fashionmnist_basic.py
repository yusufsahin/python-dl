import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import datasets, transforms
from torch.utils.data import DataLoader
import matplotlib.pyplot as plt


# ğŸŒŸ HYPERPARAMETRELER
BATCH_SIZE = 64          # Her iterasyonda iÅŸlenecek Ã¶rnek sayÄ±sÄ±
EPOCHS = 20              # EÄŸitim setinin modelden kaÃ§ kez geÃ§irileceÄŸi
LEARNING_RATE = 0.001    # Ã–ÄŸrenme oranÄ± (Ã§ok bÃ¼yÃ¼kse model kararsÄ±z olur)
USE_L2 = True            # L2 regularization (weight decay) aÃ§/kapa
USE_DROPOUT = True       # Dropout kullanÄ±lsÄ±n mÄ±?

# ğŸ§¹ VERÄ° DÃ–NÃœÅÃœMÃœ (Normalizasyon yapÄ±lÄ±r: ortalama=0, std=1)
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.5,), (0.5,))
])
# ğŸ“¥ EÄÄ°TÄ°M ve TEST VERÄ°LERÄ°NÄ° YÃœKLE (FashionMNIST)
train_data = datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)
test_data = datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)

train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)
test_loader = DataLoader(test_data, batch_size=BATCH_SIZE)

# ğŸ§  MLP MODELÄ° TANIMI (2 gizli katman, dropout dahil)
class MLP(nn.Module):
    def __init__(self):
        super(MLP, self).__init__()
        self.model = nn.Sequential(
            nn.Flatten(),                        # 28x28 â†’ 784 dÃ¼z vektÃ¶r
            nn.Linear(28 * 28, 256),             # GiriÅŸ â†’ 256 nÃ¶ron
            nn.ReLU(),                           # Aktivasyon fonksiyonu
            nn.Dropout(0.5) if USE_DROPOUT else nn.Identity(),  # Rastgele %50 nÃ¶ron devre dÄ±ÅŸÄ±
            nn.Linear(256, 128),                 # 2. gizli katman
            nn.ReLU(),                           # Aktivasyon
            nn.Linear(128, 10)                   # Ã‡Ä±kÄ±ÅŸ (10 sÄ±nÄ±f)
        )

    def forward(self, x):
        return self.model(x)

model = MLP()

# âš™ï¸ KAYIP FONKSÄ°YONU VE OPTÄ°MÄ°ZER
criterion = nn.CrossEntropyLoss()  # Ã‡ok sÄ±nÄ±flÄ± sÄ±nÄ±flandÄ±rma iÃ§in uygun
optimizer = optim.Adam(
    model.parameters(),
    lr=LEARNING_RATE,
    weight_decay=0.01 if USE_L2 else 0.0  # L2 regularization aktifse cezalandÄ±rma uygulanÄ±r
)
# ğŸ“ˆ EÄÄ°TÄ°M DÃ–NGÃœSÃœ
train_losses, test_losses = [], []

for epoch in range(EPOCHS):  # ğŸŒ Epoch = tÃ¼m eÄŸitim verisinin modelden 1 kez geÃ§mesi
    model.train()  # EÄŸitim moduna geÃ§ (dropout aktif olur)
    total_loss = 0

    for images, labels in train_loader:
        optimizer.zero_grad()             # Gradientâ€™leri sÄ±fÄ±rla
        outputs = model(images)           # Tahmin yap (forward pass)
        loss = criterion(outputs, labels) # KayÄ±p hesapla
        loss.backward()                   # Gradient hesapla (backpropagation)
        optimizer.step()                  # AÄŸÄ±rlÄ±klarÄ± gÃ¼ncelle
        total_loss += loss.item()         # KayÄ±p toplanÄ±r

    avg_train_loss = total_loss / len(train_loader)
    train_losses.append(avg_train_loss)

    # ğŸ” TEST AÅAMASI (dropout devre dÄ±ÅŸÄ±)
    model.eval()
    total_test_loss = 0
    correct = 0

    with torch.no_grad():  # Test sÄ±rasÄ±nda gradient hesaplama devre dÄ±ÅŸÄ±
        for images, labels in test_loader:
            outputs = model(images)
            loss = criterion(outputs, labels)
            total_test_loss += loss.item()

            preds = outputs.argmax(dim=1)              # En yÃ¼ksek skora sahip sÄ±nÄ±f
            correct += (preds == labels).sum().item()  # DoÄŸru tahminleri say

    avg_test_loss = total_test_loss / len(test_loader)
    test_losses.append(avg_test_loss)
    acc = correct / len(test_loader.dataset)

    # ğŸ”Š Performans Ã§Ä±ktÄ±sÄ±
    print(f"Epoch {epoch+1}/{EPOCHS} - Train Loss: {avg_train_loss:.4f} - Val Loss: {avg_test_loss:.4f} - Acc: {acc:.4f}")
# ğŸ“Š KAYIP GRAFÄ°ÄÄ° (Overfit kontrolÃ¼)
plt.plot(train_losses, label='Train Loss')
plt.plot(test_losses, label='Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.title('EÄŸitim vs DoÄŸrulama KaybÄ±')
plt.grid(True)
plt.show()