import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import datasets, transforms
from torch.utils.data import DataLoader
import matplotlib.pyplot as plt
import numpy as np
from sklearn.metrics import confusion_matrix
import seaborn as sns

# ===========================
# ğŸ”§ HÄ°PERPARAMETRELER
# ===========================
BATCH_SIZE = 64              # Her iterasyonda kullanÄ±lacak veri adedi
EPOCHS = 50                  # Toplam eÄŸitim turu (epoch) sayÄ±sÄ±
LEARNING_RATE = 0.001        # Ã–ÄŸrenme oranÄ± (learning rate)
USE_DROPOUT = True           # Dropout katmanÄ± kullanÄ±lsÄ±n mÄ±?
USE_L2 = True                # L2 regularizasyonu kullanÄ±lacak mÄ±?
USE_BATCHNORM = True         # BatchNorm katmanÄ± kullanÄ±lacak mÄ±?
OPTIMIZER_TYPE = "Adam"      # Hangi optimizasyon algoritmasÄ± kullanÄ±lacak?
WEIGHT_INIT = "He"           # AÄŸÄ±n aÄŸÄ±rlÄ±klarÄ± nasÄ±l baÅŸlatÄ±lacak?
USE_EARLYSTOP = True         # Erken durdurma (early stopping) aktif mi?
EARLYSTOP_PATIENCE = 3       # Erken durdurmada sabredilecek epoch sayÄ±sÄ±

# ===========================
# ğŸ“¥ VERÄ° HAZIRLAMA
# ===========================
# Girdi verisini normalize et ve tensÃ¶re Ã§evir
transform = transforms.Compose([
    transforms.ToTensor(),                         # GÃ¶rÃ¼ntÃ¼yÃ¼ tensÃ¶re Ã§evir (0-1)
    transforms.Normalize((0.5,), (0.5,)) # -1 ile 1 arasÄ±na normalize et
])
# FashionMNIST veri setini indir ve dÃ¶nÃ¼ÅŸtÃ¼r
train_data = datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)
test_data = datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)
# Veriyi DataLoader ile batch'lere bÃ¶l
train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)
test_loader = DataLoader(test_data, batch_size=BATCH_SIZE)

# ===========================
# ğŸ§  MODEL TANIMI
# ===========================
class AdvancedMLP(nn.Module):
    def __init__(self):
        super().__init__()
        layers = [nn.Flatten()]                              # 28x28 resmi tek boyuta indir (784,)
        layers.append(nn.Linear(28 * 28, 256))               # Ä°lk tam baÄŸlÄ± katman
        if USE_BATCHNORM:
            layers.append(nn.BatchNorm1d(256))               # Batch Normalization uygula
        layers.append(nn.ReLU())                             # Aktivasyon fonksiyonu
        if USE_DROPOUT:
            layers.append(nn.Dropout(0.5))                   # Dropout uygula (%50)
        layers.append(nn.Linear(256, 128))                   # Ä°kinci katman
        if USE_BATCHNORM:
            layers.append(nn.BatchNorm1d(128))
        layers.append(nn.ReLU())
        layers.append(nn.Linear(128, 10))                    # Ã‡Ä±kÄ±ÅŸ katmanÄ± (10 sÄ±nÄ±f)
        self.model = nn.Sequential(*layers)                  # KatmanlarÄ± sÄ±rayla baÄŸla

    def forward(self, x):
        return self.model(x)                                 # Ä°leri geÃ§iÅŸ (forward) fonksiyonu

model = AdvancedMLP()                                        # Modeli oluÅŸtur
# ===========================
# ğŸ§¬ AÄIRLIK BAÅLATMA
# ===========================
def init_weights(m):
    if isinstance(m, nn.Linear):
        if WEIGHT_INIT == "Xavier":
            nn.init.xavier_uniform_(m.weight)                # Xavier baÅŸlatma
        elif WEIGHT_INIT == "He":
            nn.init.kaiming_uniform_(m.weight, nonlinearity='relu')  # He baÅŸlatma
        nn.init.zeros_(m.bias)                               # Bias'Ä± sÄ±fÄ±rla

model.apply(init_weights)                                    # TÃ¼m katmanlara uygula
# ===========================
# âš™ï¸ KAYIP FONKSÄ°YONU & OPTÄ°MÄ°ZER
# ===========================
criterion = nn.CrossEntropyLoss()                            # Ã‡oklu sÄ±nÄ±flar iÃ§in uygun kayÄ±p fonksiyonu
weight_decay = 0.01 if USE_L2 else 0.0                      # L2 regularizasyonu (weight decay) parametresi

# SeÃ§ilen optimizere gÃ¶re oluÅŸtur
if OPTIMIZER_TYPE == "SGD":
    optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=0.9, weight_decay=weight_decay)
elif OPTIMIZER_TYPE == "RMSprop":
    optimizer = optim.RMSprop(model.parameters(), lr=LEARNING_RATE, weight_decay=weight_decay)
else:
    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=weight_decay)

# ===========================
# ğŸ§ª EÄÄ°TÄ°M DÃ–NGÃœSÃœ
# ===========================
train_losses, test_losses = [], []        # KayÄ±p deÄŸerlerini saklamak iÃ§in listeler
best_val_loss = float('inf')              # Erken durdurma iÃ§in en iyi doÄŸrulama kaybÄ±
patience_counter = 0                      # SabÄ±r sayacÄ± (erken durdurma)

for epoch in range(EPOCHS):
    model.train()                         # Modeli eÄŸitim moduna al
    total_train_loss = 0

    for images, labels in train_loader:
        optimizer.zero_grad()             # Gradientleri sÄ±fÄ±rla
        outputs = model(images)           # Tahminleri al
        loss = criterion(outputs, labels) # KayÄ±p hesapla
        loss.backward()                   # Gradientleri hesapla (geri yayÄ±lÄ±m)
        optimizer.step()                  # AÄŸÄ±rlÄ±klarÄ± gÃ¼ncelle
        total_train_loss += loss.item()   # Toplam eÄŸitim kaybÄ±na ekle

    avg_train_loss = total_train_loss / len(train_loader)    # Ortalama eÄŸitim kaybÄ±
    train_losses.append(avg_train_loss)

    model.eval()                          # Modeli deÄŸerlendirme moduna al
    total_test_loss = 0
    correct = 0

    with torch.no_grad():                 # Gradient hesaplama kapalÄ±
        for images, labels in test_loader:
            outputs = model(images)
            loss = criterion(outputs, labels)
            total_test_loss += loss.item()
            preds = outputs.argmax(dim=1)             # En yÃ¼ksek olasÄ±lÄ±klÄ± sÄ±nÄ±fÄ± seÃ§
            correct += (preds == labels).sum().item() # DoÄŸru tahmin sayÄ±sÄ±nÄ± topla

    avg_test_loss = total_test_loss / len(test_loader)      # Ortalama doÄŸrulama kaybÄ±
    test_losses.append(avg_test_loss)
    acc = correct / len(test_loader.dataset)                # DoÄŸruluk (accuracy)

    print(f"Epoch {epoch+1}/{EPOCHS} - Train Loss: {avg_train_loss:.4f} - Val Loss: {avg_test_loss:.4f} - Acc: {acc:.4f}")

    # Erken Durdurma (Early Stopping) kontrolÃ¼
    if USE_EARLYSTOP:
        if avg_test_loss < best_val_loss:
            best_val_loss = avg_test_loss
            patience_counter = 0
        else:
            patience_counter += 1
            if patience_counter >= EARLYSTOP_PATIENCE:
                print(f"ğŸ›‘ Early stopping at epoch {epoch+1}")
                break
# ===========================
# ğŸ“‰ KAYIP GRAFÄ°KLERÄ°
# ===========================
plt.plot(train_losses, label='Train Loss')           # EÄŸitim kaybÄ± grafiÄŸi
plt.plot(test_losses, label='Validation Loss')       # DoÄŸrulama kaybÄ± grafiÄŸi
plt.xlabel('Epoch')                                 # X ekseni: Epoch
plt.ylabel('Loss')                                  # Y ekseni: KayÄ±p
plt.title('EÄŸitim vs DoÄŸrulama KaybÄ±')              # Grafik baÅŸlÄ±ÄŸÄ±
plt.legend()                                        # AÃ§Ä±klamalar
plt.grid(True)                                      # Izgara
plt.show()                                          # GÃ¶ster

# ===========================
# ğŸ“Š CONFUSION MATRIX (KARIÅIKLIK MATRÄ°SÄ°)
# ===========================
all_preds = []
all_labels = []

model.eval()                                        # Modeli deÄŸerlendirme moduna al
with torch.no_grad():
    for images, labels in test_loader:
        outputs = model(images)
        preds = outputs.argmax(dim=1)
        all_preds.extend(preds.numpy())              # Tahmin edilen sÄ±nÄ±flarÄ± topla
        all_labels.extend(labels.numpy())            # GerÃ§ek sÄ±nÄ±flarÄ± topla

cm = confusion_matrix(all_labels, all_preds)         # Confusion matrix hesapla
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues")  # GÃ¶rselleÅŸtir
plt.xlabel("Tahmin")                                # X ekseni etiketi
plt.ylabel("GerÃ§ek")                                # Y ekseni etiketi
plt.title("Confusion Matrix")                       # BaÅŸlÄ±k
plt.show()

# ===========================
# ğŸ’¾ MODELÄ° KAYDETME (Opsiyonel)
# ===========================
# torch.save(model.state_dict(), "advanced_mlp.pt")  # Modelin aÄŸÄ±rlÄ±klarÄ±nÄ± kaydet
# print("Model kaydedildi: advanced_mlp.pt")